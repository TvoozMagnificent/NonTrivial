---
layout: default
title: What do you mean by "impact"???
customjs: https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML
---

> **Thought experiment:** Imagine you want to develop cancer treatments after a loved one's illness. But you have great engineering skills that could advance clean energy. How would you weigh following your passions against contributing where your talents may have outsized impact?
>
> **How would you weigh passion against the potential to help more people through another cause? Are there certain causes you want to support regardless of scale?**

Even though having potential in a particular area of expertise can be very beneficial, **passion is a more important factor**. Having passion towards something can help you gain motivation towards doing your work. In fact, contributions are only possible when **passion aligns with potential**. Therefore, when the two differ, it is important that one of them, either passion or potential, shifts towards the other. Usually, passion can be hard to shift, but expertise can always be acquired. 

Of course, the deciding factor always contains **the specifics of the contribution**. Without knowledge of the specifics, it can be hard to decide on the better result. For instance, how much the cause can benefit the world can also determine whether it is worth working for. 

> An equation for counterfactual impact: amount of good you did $-$ what would have happened without you. 
>
> **Do you care more about having a counterfactual impact or direct impact? Why?**

I care more about creating a positive influence towards the society. This is best represented as a **counterfactual impact** since influence is characterized as the effect, or **sway** of one's action, which is precisely the definition of counterfactual impact. 

Overall, the point of "doing good" to the society is not just about your direct impact or your emotional well-being, but to create a **net positive** (i.e. better) scenario. If I could save somebody's life, but the doctor beside me could have done better, I would not commit to the act just to gain social acceptance. 

> **Thought experiment:** Imagine on your way to a concert, you come across a shallow pond and notice a child who has fallen in and is drowning. You could easily wade in and save the child. But you‚Äôd ruin your favourite clothes (which cost a few thousand dollars)!
> 
> Do you have an obligation to rescue the child? And what if other people are walking past the pond and choosing not to intervene, does this change your obligation to act?
> 
> Imagine you could save a child's life overseas by donating money ([about $4,500](https://www.givewell.org/cost-to-save-a-life)). Some argue we have an obligation to help others in need regardless of distance. Others disagree, saying that such an obligation would be too demanding, we have special obligations to our community, or that donations don‚Äôt address the systemic causes of global inequality. What do you think?
>
> **Do you believe we have an obligation to help others in need? Why or why not?**

Obligation is strictly defined as [a course of action that someone is required to take, whether legal or moral](https://en.wikipedia.org/wiki/Obligation). It is clear that the law is the red line of morality. Under most circumstances, one does not have a legal obligation to help others in need, especially in the **Thought Experiment**. However, many people can feel morally obligated to save the child, even if other people are around. Of course, if there is a lifeguard around, one would notify him or her instead of taking actions immediately. However, most people will not consider always donating $4,500 to save a child's life rational, much less an **obligation**. Reasons may include: 

1. Low **transitivity**. It is harder to see the direct effect of a $4,500 donation towards a charity. However, ruining your clothes has the direct effect of saving that one specific child that you see is drowning. 
2. Low **traceability**. Very few charities identify specifically whom your donation goes towards. In the worst case scenario, it could even be a scam. This means that you do not know if your donation was actually helpful. 
3. Low **tractability**. Sacrificing clothing for a child's life right in front of you can be instinctive, but the amount of procedures for a single donation can take time. This can deter a lot of potential donors.
4. Low **time limit**. Saving a drowning child right in front of you is a very time-intensive task. If you make the decision later, the child might end up getting killed. This inflexibility in time compared to donations can make decisions more impulsive. 

> **How do you decide when helping others requires too high a personal cost? How would you know if you were giving up too much?**

The best way is to look at the **most pressing problems** that we face. Altruism believes in [the selfless concern for the well-being of others](https://www.oed.com/dictionary/altruism_n?tab=factsheet#5841125). Not helping others is not about the personal cost being too high. In contrary, it is about dedicating more time and energy to other things that requires it. Using one minute of your time to save another person's life might sound like a fantastic idea, but I would turn it down if, in the same time, I could save two lives instead. **Helping others is too much if it is not maximally efficient.** 

> **Thought experiment:** On your way to school in the morning, you watch a white van pull up next to a child a hundred metres away. Someone slides the back door of the van open, and quickly snatches the child. You run after them, but they drive off before you get there. There was no one else on the street, so you‚Äôre not sure if anyone else saw. Should you help this child by reporting it to the police? Does your obligation change if, instead, you see a kidnapping on a livestream of a public square in another country?
> 
> **Do you think we have a moral obligation to help others who are in the same community as us, even if we can help less than we can other people?**

**No, we don't.** In fact, a good community will consist of people that are willing to sacrifice themselves if it means doing more good overall (i.e. positive counterfactual impact). 

> **Thought experiment:** The majority of people in western democracies eat meat, often from animals kept in inhumane conditions like being confined to crates not much larger than themselves.
>
> Yet, many philosophers and neuroscientists [believe](https://fcmconference.org/img/CambridgeDeclarationOnConsciousness.pdf) that animals have the capacity to experience pleasure and pain. How would you assess the ethics of this practice?
>
> **Are non-human animals worthy of our concern? What makes a species worthy of our concern vs not?**

I agree with the viewpoint that a species is worthy of our concern if it can **feel pain**. 

> **Imagine a trolley was hurtling down the track and you had to choose between it killing a human or hitting $x$ chickens. Is there a number of chickens where you would choose to save their life over a human? What do you think is the smallest number of chickens where you would choose them over the human?**

For reasonably small $x$ (say, less than a million), **I would choose to save a human life.** However, once $x$ gets increasingly large (say, approaching $1\%$ of their entire population) I would save the chickens' lives in order to not impact biology. 

That being said, actions will be taken towards minimizing the amount of damage done, for instance by utilizing painkillers. 

> **Thought experiment:** You're a lifeguard at a busy beach when you notice a rare tide has resulted in 1,000 people being drawn out to sea. 
> 
> You have two options, and only have time for one of them:
> 1.  Run in and save 1 person who you'll have to choose randomly, **OR**
> 2.  Call the Coast Guard, who can save everyone. But, there's only a 1% chance they're in the area, so there's a 1% chance of saving
> everyone.
> 
> Should you help a guaranteed amount or take a risk of helping more people?
>
> **How comfortable are you with tackling problems that have uncertain solutions or impact?**

Ultimately, such problems end up being a game about expected value. I believe that comparing $\mathbb E[$lives saved$]$ is **philosophically correct**. However, one has to be careful about hidden variables in the scenario. For instance, $1\%$ may not be a very accurate estimate. Additionally, saving only 1 person may cause emotional damage to that person. That person might lose his friends and family, and even feel guilty of not saving more people. 

> **You have two options:**
> 
> 1.  **50% chance of saving 20 lives.**
> 2.  **100% chance of saving $x$ lives.**
> 
> **What‚Äôs the smallest value of $x$ you‚Äôd accept by taking option 2?**

Ignoring hidden variables for now (since no specification is given), we'll apply the discrete formula for calculating expected values. 

$$\begin{align*}\mathbb E[\text{Option 2}] &\ge \mathbb E[\text{Option 1}]\\
\text{Pr}[\text{Success}_\text{\ Option 2}] \cdot x &\ge 
\text{Pr}[\text{Success}_\text{\ Option 1}] \cdot 20\\
100\%\cdot x &\ge 50\%\cdot 20\\
x &\ge 10\end{align*}$$

Therefore, the minimum accepted value $x_\text{min}=10$. 

> **If you use expected value, how will you avoid [Pascal's mugging](https://forum.effectivealtruism.org/topics/pascal-s-mugging)?**

Simple: there is a hidden variable involved. 

If a person walks up to you and threatens killing one person, is (s)he telling the truth? Maybe. We'll overestimate and say $\text{Pr[\text{truth}]}=0.1$. This factor will be considered when you think about whether to hand in your wallet. 

What if the person walks up to you and threatens killing $10$ people? Well, we would expect $\text{Pr[\text{truth}]}=0.01$ or even less. 

What if the person walks up to you and threatens killing $10^{10}$ people? Well, we know that $\text{Pr[\text{truth}]}=0$ (effectively). 

There are two key points here: 

First, the expected value of the outcome doesn't always increase when the number of people promised to be killed increases. If anything, the more exaggerated the statement is (or seems to be), the less likely it is true, and the less like we'd have to consider it. This is a **hidden variable**. 

Second, there simply cannot be a scenario where "arbitrarily increasing something" really works. At some point, it will start becoming implausible, and then slowly **impossible**. 

> **Which ethical framework(s) do you think an ideal moral person should follow? What are the advantages and disadvantages of your choice?**

I think that utilitarianism is the most ideal ethical framework. As humans, we know that **our decisions have consequences**. Ultimately, it is not about how we use our power, but what we use our power for. The advantage is obvious: you are less likely to be stuck between a hard place, less likely to be caught in an inescapable moral dilemma, for you know you do not have to the morally correct thing, only that the result has to be morally correct. 

Of course, its disadvantage is that once other people take your utilitarianism into account of their own decisions, then your predictions may become off track. As a bridge player, I have a few experiences with calls that do not serve for the purpose they normally serve. 

```
South: 
S Q75
H Q63
D Q9432
C 52

  N     E     S     W
1NT     /    2C
```

Here, South is nowhere near the requirements for the `2C` Stayman call, with no 4M or 8 HCP. However, since North is forced to call `2D`, `2H`, or `2S`, South can safely pass out either of these. Any of these will be better than passing out `1NT`. 

*[4M]: 4-card major suit
*[HCP]: High Card Point

> **In your [moral parliament](https://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html) to account for [moral uncertainty](https://forum.effectivealtruism.org/topics/moral-uncertainty#fno3rr9ulfig), how many votes out of 100 should you devote to each framework?**

I would say 0, 100, and 0, respectively, since moral uncertainty is hard to deal with and can be very inconsistent. 

> **What are your thoughts on if and how the welfare of future people should factor into current choices?**

I think that they are **as important**. The situation is the same in both cases. 

> **Imagine you‚Äôre contacted by an alien who asks you to choose between three possible outcomes for humanity:**
> 1.  **A world at peace**
> 2.  **They kill 99% of the world‚Äôs existing population.**
> 3.  **A nuclear war that kills 100% of the world's population, resulting in the extinction of the human race.**
> 
> **Is option 3 worse than option 2? If so, how much worse? Why/why not?**

Option 3 is infinitely worse than option 2, since option 3 means that nobody can have any more impact. However, in option 2, there is still a chance for human civilization to rebuild itself. 

> **What else are you uncertain about what you value?**

Abstract things like the will to achieve self-actualization. 

# Summary

Type of impact|Best guess
-|-
[Interest-driven](https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.y1dj8yxkjftd)|üí• I want to have the biggest impact and I'm open to new areas
[Counterfactual](https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.2ew8vg41o8ji)|‚öñÔ∏è Compare to what would have happened (counterfactual adjustment)
[Obligations](https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.wnehuvezh95c)|üôÖ‚Äç‚ôÄÔ∏è We should help others even at significant personal cost. However, note that we also have to consider the counterfactual adjustment:  If I do not take this current chance to do good, can I allow myself to do more good later?
[Geography](https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.m609w4n8zqjw)|üåê All people matter equally (cosmopolitanism)
[Species](https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.w80j0lnbbm0s)|üêæ All species deserve concern. Of course, there are many reasons why humans deserve more concern.
[Uncertainty](https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.vjjaor5xzu6g)|‚ò¢Ô∏è Use risk-neutral expected utility theory (most risk)
[Ethical framework](https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.otvks3n1n43u)|‚≠ê Consequences are all that matter (consequentialism e.g. utilitarianism)
[Future generations](https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.asijj7iw35l5)|üåå Future generations are of overwhelming importance
