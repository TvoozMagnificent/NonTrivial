<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>What do you mean by "impact"?</title>
  <!-- MathJax -->
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>

<body>
<blockquote>
<p><strong>Thought experiment:</strong> Imagine you want to develop
cancer treatments after a loved one‚Äôs illness. But you have great
engineering skills that could advance clean energy. How would you weigh
following your passions against contributing where your talents may have
outsized impact?</p>
<p><strong>How would you weigh passion against the potential to help
more people through another cause? Are there certain causes you want to
support regardless of scale?</strong></p>
</blockquote>
<p>Even though having potential in a particular area of expertise can be
very beneficial, <strong>passion is a more important factor</strong>.
Having passion towards something can help you gain motivation towards
doing your work. In fact, contributions are only possible when
<strong>passion aligns with potential</strong>. Therefore, when the two
differ, it is important that one of them, either passion or potential,
shifts towards the other. Usually, passion can be hard to shift, but
expertise can always be acquired.</p>
<p>Of course, the deciding factor always contains <strong>the specifics
of the contribution</strong>. Without knowledge of the specifics, it can
be hard to decide on the better result. For instance, how much the cause
can benefit the world can also determine whether it is worth working
for.</p>
<blockquote>
<p>An equation for counterfactual impact: amount of good you did <span
class="math inline">‚àí</span> what would have happened without you.</p>
<p><strong>Do you care more about having a counterfactual impact or
direct impact? Why?</strong></p>
</blockquote>
<p>I care more about creating a positive influence towards the society.
This is best represented as a <strong>counterfactual impact</strong>
since influence is characterized as the effect, or <strong>sway</strong>
of one‚Äôs action, which is precisely the definition of counterfactual
impact.</p>
<p>Overall, the point of ‚Äúdoing good‚Äù to the society is not just about
your direct impact or your emotional well-being, but to create a
<strong>net positive</strong> (i.e. better) scenario. If I could save
somebody‚Äôs life, but the doctor beside me could have done better, I
would not commit to the act just to gain social acceptance.</p>
<blockquote>
<p><strong>Thought experiment:</strong> Imagine on your way to a
concert, you come across a shallow pond and notice a child who has
fallen in and is drowning. You could easily wade in and save the child.
But you‚Äôd ruin your favourite clothes (which cost a few thousand
dollars)!</p>
<p>Do you have an obligation to rescue the child? And what if other
people are walking past the pond and choosing not to intervene, does
this change your obligation to act?</p>
<p>Imagine you could save a child‚Äôs life overseas by donating money (<a
href="https://www.givewell.org/cost-to-save-a-life">about $4,500</a>).
Some argue we have an obligation to help others in need regardless of
distance. Others disagree, saying that such an obligation would be too
demanding, we have special obligations to our community, or that
donations don‚Äôt address the systemic causes of global inequality. What
do you think?</p>
<p><strong>Do you believe we have an obligation to help others in need?
Why or why not?</strong></p>
</blockquote>
<p>Obligation is strictly defined as <a
href="https://en.wikipedia.org/wiki/Obligation">a course of action that
someone is required to take, whether legal or moral</a>. It is clear
that the law is the red line of morality. Under most circumstances, one
does not have a legal obligation to help others in need, especially in
the <strong>Thought Experiment</strong>. However, many people can feel
morally obligated to save the child, even if other people are around. Of
course, if there is a lifeguard around, one would notify him or her
instead of taking actions immediately. However, most people will not
consider always donating $4,500 to save a child‚Äôs life rational, much
less an <strong>obligation</strong>. Reasons may include:</p>
<ol type="1">
<li>Low <strong>transitivity</strong>. It is harder to see the direct
effect of a $4,500 donation towards a charity. However, ruining your
clothes has the direct effect of saving that one specific child that you
see is drowning.</li>
<li>Low <strong>traceability</strong>. Very few charities identify
specifically whom your donation goes towards. In the worst case
scenario, it could even be a scam. This means that you do not know if
your donation was actually helpful.</li>
<li>Low <strong>tractability</strong>. Sacrificing clothing for a
child‚Äôs life right in front of you can be instinctive, but the amount of
procedures for a single donation can take time. This can deter a lot of
potential donors.</li>
<li>Low <strong>time limit</strong>. Saving a drowning child right in
front of you is a very time-intensive task. If you make the decision
later, the child might end up getting killed. This inflexibility in time
compared to donations can make decisions more impulsive.</li>
</ol>
<blockquote>
<p><strong>How do you decide when helping others requires too high a
personal cost? How would you know if you were giving up too
much?</strong></p>
</blockquote>
<p>The best way is to look at the <strong>most pressing
problems</strong> that we face. Altruism believes in <a
href="https://www.oed.com/dictionary/altruism_n?tab=factsheet#5841125">the
selfless concern for the well-being of others</a>. Not helping others is
not about the personal cost being too high. In contrary, it is about
dedicating more time and energy to other things that requires it. Using
one minute of your time to save another person‚Äôs life might sound like a
fantastic idea, but I would turn it down if, in the same time, I could
save two lives instead. <strong>Helping others is too much if it is not
maximally efficient.</strong></p>
<blockquote>
<p><strong>Thought experiment:</strong> On your way to school in the
morning, you watch a white van pull up next to a child a hundred metres
away. Someone slides the back door of the van open, and quickly snatches
the child. You run after them, but they drive off before you get there.
There was no one else on the street, so you‚Äôre not sure if anyone else
saw. Should you help this child by reporting it to the police? Does your
obligation change if, instead, you see a kidnapping on a livestream of a
public square in another country?</p>
<p><strong>Do you think we have a moral obligation to help others who
are in the same community as us, even if we can help less than we can
other people?</strong></p>
</blockquote>
<p><strong>No, we don‚Äôt.</strong> In fact, a good community will consist
of people that are willing to sacrifice themselves if it means doing
more good overall (i.e. positive counterfactual impact).</p>
<blockquote>
<p><strong>Thought experiment:</strong> The majority of people in
western democracies eat meat, often from animals kept in inhumane
conditions like being confined to crates not much larger than
themselves.</p>
<p>Yet, many philosophers and neuroscientists <a
href="https://fcmconference.org/img/CambridgeDeclarationOnConsciousness.pdf">believe</a>
that animals have the capacity to experience pleasure and pain. How
would you assess the ethics of this practice?</p>
<p><strong>Are non-human animals worthy of our concern? What makes a
species worthy of our concern vs not?</strong></p>
</blockquote>
<p>I agree with the viewpoint that a species is worthy of our concern if
it can <strong>feel pain</strong>.</p>
<blockquote>
<p><strong>Imagine a trolley was hurtling down the track and you had to
choose between it killing a human or hitting <span
class="math inline"><em>x</em></span> chickens. Is there a number of
chickens where you would choose to save their life over a human? What do
you think is the smallest number of chickens where you would choose them
over the human?</strong></p>
</blockquote>
<p>For reasonably small <span class="math inline"><em>x</em></span>
(say, less than a million), <strong>I would choose to save a human
life.</strong> However, once <span class="math inline"><em>x</em></span>
gets increasingly large (say, approaching <span
class="math inline">1%</span> of their entire population) I would save
the chickens‚Äô lives in order to not impact biology.</p>
<p>That being said, actions will be taken towards minimizing the amount
of damage done, for instance by utilizing painkillers.</p>
<blockquote>
<p><strong>Thought experiment:</strong> You‚Äôre a lifeguard at a busy
beach when you notice a rare tide has resulted in 1,000 people being
drawn out to sea.</p>
<p>You have two options, and only have time for one of them: 1. Run in
and save 1 person who you‚Äôll have to choose randomly,
<strong>OR</strong> 2. Call the Coast Guard, who can save everyone. But,
there‚Äôs only a 1% chance they‚Äôre in the area, so there‚Äôs a 1% chance of
saving everyone.</p>
<p>Should you help a guaranteed amount or take a risk of helping more
people?</p>
<p><strong>How comfortable are you with tackling problems that have
uncertain solutions or impact?</strong></p>
</blockquote>
<p>Ultimately, such problems end up being a game about expected value. I
believe that comparing <span class="math inline">ùîº[</span>lives
saved<span class="math inline">]</span> is <strong>philosophically
correct</strong>. However, one has to be careful about hidden variables
in the scenario. For instance, <span class="math inline">1%</span> may
not be a very accurate estimate. Additionally, saving only 1 person may
cause emotional damage to that person. That person might lose his
friends and family, and even feel guilty of not saving more people.</p>
<blockquote>
<p><strong>You have two options:</strong></p>
<ol type="1">
<li><strong>50% chance of saving 20 lives.</strong></li>
<li><strong>100% chance of saving <span
class="math inline"><em>x</em></span> lives.</strong></li>
</ol>
<p><strong>What‚Äôs the smallest value of <span
class="math inline"><em>x</em></span> you‚Äôd accept by taking option
2?</strong></p>
</blockquote>
<p>Ignoring hidden variables for now (since no specification is given),
we‚Äôll apply the discrete formula for calculating expected values.</p>
<p><span class="math display">$$\begin{align*}\mathbb E[\text{Option 2}]
&amp;\ge \mathbb E[\text{Option 1}]\\
\text{Pr}[\text{Success}_\text{\ Option 2}] \cdot x &amp;\ge
\text{Pr}[\text{Success}_\text{\ Option 1}] \cdot 20\\
100\%\cdot x &amp;\ge 50\%\cdot 20\\
x &amp;\ge 10\end{align*}$$</span></p>
<p>Therefore, the minimum accepted value <span
class="math inline"><em>x</em><sub>min</sub>‚ÄÑ=‚ÄÑ10</span>.</p>
<blockquote>
<p><strong>If you use expected value, how will you avoid <a
href="https://forum.effectivealtruism.org/topics/pascal-s-mugging">Pascal‚Äôs
mugging</a>?</strong></p>
</blockquote>
<p>Simple: there is a hidden variable involved.</p>
<p>If a person walks up to you and threatens killing one person, is
(s)he telling the truth? Maybe. We‚Äôll overestimate and say <span
class="math inline">$\text{Pr[\text{truth}]}=0.1$</span>. This factor
will be considered when you think about whether to hand in your
wallet.</p>
<p>What if the person walks up to you and threatens killing <span
class="math inline">10</span> people? Well, we would expect <span
class="math inline">$\text{Pr[\text{truth}]}=0.01$</span> or even
less.</p>
<p>What if the person walks up to you and threatens killing <span
class="math inline">10<sup>10</sup></span> people? Well, we know that
<span class="math inline">$\text{Pr[\text{truth}]}=0$</span>
(effectively).</p>
<p>There are two key points here:</p>
<p>First, the expected value of the outcome doesn‚Äôt always increase when
the number of people promised to be killed increases. If anything, the
more exaggerated the statement is (or seems to be), the less likely it
is true, and the less like we‚Äôd have to consider it. This is a
<strong>hidden variable</strong>.</p>
<p>Second, there simply cannot be a scenario where ‚Äúarbitrarily
increasing something‚Äù really works. At some point, it will start
becoming implausible, and then slowly <strong>impossible</strong>.</p>
<blockquote>
<p><strong>Which ethical framework(s) do you think an ideal moral person
should follow? What are the advantages and disadvantages of your
choice?</strong></p>
</blockquote>
<p>I think that utilitarianism is the most ideal ethical framework. As
humans, we know that <strong>our decisions have consequences</strong>.
Ultimately, it is not about how we use our power, but what we use our
power for. The advantage is obvious: you are less likely to be stuck
between a hard place, less likely to be caught in an inescapable moral
dilemma, for you know you do not have to the morally correct thing, only
that the result has to be morally correct.</p>
<p>Of course, its disadvantage is that once other people take your
utilitarianism into account of their own decisions, then your
predictions may become off track. As a bridge player, I have a few
experiences with calls that do not serve for the purpose they normally
serve.</p>
<pre><code>South: 
S Q75
H Q63
D Q9432
C 52

  N     E     S     W
1NT     /    2C</code></pre>
<p>Here, South is nowhere near the requirements for the <code>2C</code>
Stayman call, with no 4M or 8 HCP. However, since North is forced to
call <code>2D</code>, <code>2H</code>, or <code>2S</code>, South can
safely pass out either of these. Any of these will be better than
passing out <code>1NT</code>.</p>
<p><em>[4M]: 4-card major suit </em>[HCP]: High Card Point</p>
<blockquote>
<p><strong>In your <a
href="https://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html">moral
parliament</a> to account for <a
href="https://forum.effectivealtruism.org/topics/moral-uncertainty#fno3rr9ulfig">moral
uncertainty</a>, how many votes out of 100 should you devote to each
framework?</strong></p>
</blockquote>
<p>I would say 0, 100, and 0, respectively, since moral uncertainty is
hard to deal with and can be very inconsistent.</p>
<blockquote>
<p><strong>What are your thoughts on if and how the welfare of future
people should factor into current choices?</strong></p>
</blockquote>
<p>I think that they are <strong>as important</strong>. The situation is
the same in both cases.</p>
<blockquote>
<p><strong>Imagine you‚Äôre contacted by an alien who asks you to choose
between three possible outcomes for humanity:</strong> 1. <strong>A
world at peace</strong> 2. <strong>They kill 99% of the world‚Äôs existing
population.</strong> 3. <strong>A nuclear war that kills 100% of the
world‚Äôs population, resulting in the extinction of the human
race.</strong></p>
<p><strong>Is option 3 worse than option 2? If so, how much worse?
Why/why not?</strong></p>
</blockquote>
<p>Option 3 is infinitely worse than option 2, since option 3 means that
nobody can have any more impact. However, in option 2, there is still a
chance for human civilization to rebuild itself.</p>
<blockquote>
<p><strong>What else are you uncertain about what you
value?</strong></p>
</blockquote>
<p>Abstract things like the will to achieve self-actualization.</p>
<h1 id="summary">Summary</h1>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Type of impact</th>
<th>Best guess</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a
href="https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.y1dj8yxkjftd">Interest-driven</a></td>
<td>üí• I want to have the biggest impact and I‚Äôm open to new areas</td>
</tr>
<tr class="even">
<td><a
href="https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.2ew8vg41o8ji">Counterfactual</a></td>
<td>‚öñÔ∏è Compare to what would have happened (counterfactual
adjustment)</td>
</tr>
<tr class="odd">
<td><a
href="https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.wnehuvezh95c">Obligations</a></td>
<td>üôÖ‚Äç‚ôÄÔ∏è We should help others even at significant personal cost. However,
note that we also have to consider the counterfactual adjustment: If I
do not take this current chance to do good, can I allow myself to do
more good later?</td>
</tr>
<tr class="even">
<td><a
href="https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.m609w4n8zqjw">Geography</a></td>
<td>üåê All people matter equally (cosmopolitanism)</td>
</tr>
<tr class="odd">
<td><a
href="https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.w80j0lnbbm0s">Species</a></td>
<td>üêæ All species deserve concern. Of course, there are many reasons
why humans deserve more concern.</td>
</tr>
<tr class="even">
<td><a
href="https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.vjjaor5xzu6g">Uncertainty</a></td>
<td>‚ò¢Ô∏è Use risk-neutral expected utility theory (most risk)</td>
</tr>
<tr class="odd">
<td><a
href="https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.otvks3n1n43u">Ethical
framework</a></td>
<td>‚≠ê Consequences are all that matter (consequentialism
e.g. utilitarianism)</td>
</tr>
<tr class="even">
<td><a
href="https://docs.google.com/document/d/1_lyIiHL-ELb0lT4YSQ_-GanrBS6zR-6qIXhMrpZJqWo/edit#heading=h.asijj7iw35l5">Future
generations</a></td>
<td>üåå Future generations are of overwhelming importance</td>
</tr>
</tbody>
</table>
</body>
</html>
